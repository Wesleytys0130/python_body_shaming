{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d70d37",
   "metadata": {},
   "source": [
    "## 先跑函式庫！\n",
    "記得修改headers！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd829f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4, re\n",
    "import urllib.request as req\n",
    "def changedate(str):\n",
    "    if 'Jan' in str:\n",
    "        return '01'\n",
    "    elif 'Feb' in str:\n",
    "        return '02'\n",
    "    elif 'Mar' in str:\n",
    "        return '03'\n",
    "    elif 'Apr' in str:\n",
    "        return '04'\n",
    "    elif 'May' in str:\n",
    "        return '05'\n",
    "    elif 'Jun' in str:\n",
    "        return '06'\n",
    "    elif 'Jul' in str:\n",
    "        return '07'\n",
    "    elif 'Aug' in str:\n",
    "        return '08'\n",
    "    elif 'Sep' in str:\n",
    "        return '09'\n",
    "    elif 'Oct' in str:\n",
    "        return '10'\n",
    "    elif 'Nov' in str:\n",
    "        return '11'\n",
    "    elif 'Dec' in str:\n",
    "        return '12'\n",
    "\n",
    "def article(url):\n",
    "    #建立一個Request物件，附加Request Headers 的資訊\n",
    "    request = req.Request(url, headers={\"cookie\" : \"over18=1\",\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36(KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\"})\n",
    "    with req.urlopen(request) as response:\n",
    "        data = response.read().decode(\"utf-8\")\n",
    "    html = bs4.BeautifulSoup(data, \"html.parser\")\n",
    "    # artdic = {'art':{\"artID\":[],\"art_title\":[],\"art_time\":[],\"art_text\":[]},'com':{\"userID\":[], \"tag\":[], \"content\":[], \"date\":[], \"year\":[]}}\n",
    "    artdic = {\"artID\":[],\"art_title\":[],\"art_time\":[],\"art_text\":[]}\n",
    "   \n",
    "    arthtml = html.find_all(\"div\", class_=\"article-metaline\")\n",
    "    artID =re.search('作者<\\/span><span class=\"article-meta-value\">.*<\\/span><\\/div>, <div class=\"article-metaline\"><span class=\"article-meta-tag\">標題',str(arthtml))\n",
    "    art_title = re.search('標題<\\/span><span class=\"article-meta-value\">.*<\\/span><\\/div>, <div class=\"article-metaline\"><span class=\"article-meta-tag\">時間',str(arthtml))\n",
    "    art_time = re.search(' [A-Za-z]+ .*\\d',str(arthtml))\n",
    "    if artID == None or art_title == None or art_time == None:\n",
    "        print(\"None\")\n",
    "    else:\n",
    "        artID=re.sub('作者<\\/span><span class=\"article-meta-value\">',\"\",artID.group())\n",
    "        artID=re.sub('<\\/span><\\/div>, <div class=\"article-metaline\"><span class=\"article-meta-tag\">標題',\"\",artID)\n",
    "        artdic[\"artID\"].append(str(artID))\n",
    "        \n",
    "        art_title = re.sub('標題<\\/span><span class=\"article-meta-value\">',\"\",art_title.group())\n",
    "        art_title = re.sub('<\\/span><\\/div>, <div class=\"article-metaline\"><span class=\"article-meta-tag\">時間',\"\",art_title)\n",
    "        artdic[\"art_title\"].append(str(art_title))\n",
    "\n",
    "        art_time = re.sub('\\w*:\\w*:\\w* ',\"\",art_time.group())\n",
    "        mon = changedate(art_time)\n",
    "        art_time = re.sub(' [A-Z].. ',mon+'/',art_time)\n",
    "        artdic[\"art_time\"].append(str(art_time))\n",
    "\n",
    "    art_text = html.find_all(\"div\", class_=\"bbs-screen\")\n",
    "    art_text = re.sub(\"\\[<div.*<\\/span><\\/div>\\n\",\"\",str(art_text))\n",
    "    art_text = re.sub('<div class=\"push\">.*\\n<\\/span><\\/div>',\"\",art_text)\n",
    "    art_text = re.sub('<span.*\\n<\\/span>',\"\",art_text)\n",
    "    art_text = re.sub('\\n--\\n',\"\",art_text)\n",
    "    art_text = re.sub('</div>]',\"\",art_text)\n",
    "    if '<a' in art_text:\n",
    "        art_text = re.sub('<a.*<\\/a>',\"\",art_text)\n",
    "    if '<div' in art_text:\n",
    "        art_text = re.sub('<div.*<\\/div>',\"\",art_text)\n",
    "    if '<span' in art_text:\n",
    "        art_text = re.sub('<span.*<\\/span>',\"\",art_text)    \n",
    "    artdic[\"art_text\"].append(str(art_text))\n",
    "    \n",
    "    return artdic\n",
    "\n",
    "def comments(url):\n",
    "    #建立一個Request物件，附加Request Headers 的資訊\n",
    "    request = req.Request(url, headers={\"cookie\" : \"over18=1\",\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36(KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\"})\n",
    "    with req.urlopen(request) as response:\n",
    "        data = response.read().decode(\"utf-8\")\n",
    "    html = bs4.BeautifulSoup(data, \"html.parser\")\n",
    "    comdic = {\"userID\":[], \"tag\":[], \"content\":[], \"date\":[], \"year\":[]}\n",
    "    \n",
    "\n",
    "    comments=html.find_all(\"div\", class_=\"push\") #尋找所有 class=\"push\"的 div 標籤\n",
    "    year=html.find_all(\"span\", class_=\"article-meta-value\")\n",
    "    year=re.search(\":\\d\\d \\d+<\\/span>\",str(year))\n",
    "    if year == None:\n",
    "        print(year)\n",
    "    else:\n",
    "        year=re.sub(\"<\\/span>\",\"\",year.group())\n",
    "        year=re.sub(\":\\d*\",\"\",year)\n",
    "\n",
    "    for comment in comments:\n",
    "        if \"warning-box\" not in str(comment):\n",
    "            userID = comment.find(\"span\",class_=\"push-userid\").string\n",
    "            tag = comment.find(\"span\",class_=\"push-tag\").string\n",
    "            content = comment.find(\"span\",class_=\"push-content\").string\n",
    "            date = comment.find(\"span\",class_=\"push-ipdatetime\").string\n",
    "            date = re.search(\"\\d+\\/\\d+\",str(date))\n",
    "            if userID != None and tag != None and content != None and date != None:\n",
    "                comdic[\"userID\"].append(str(userID))\n",
    "\n",
    "                tag_clear = re.sub(' ','',str(tag))\n",
    "                comdic[\"tag\"].append(tag_clear)\n",
    "\n",
    "                content_clear = re.sub(': ','',str(content)) #清理留言特殊符號和空白\n",
    "                content_clear = re.sub(\"', \",'',content_clear)\n",
    "                comdic[\"content\"].append(content_clear)\n",
    "\n",
    "                comdic[\"date\"].append(date.group())\n",
    "                comdic[\"year\"].append(year)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    return comdic\n",
    "\n",
    "# arturl = 'https://www.ptt.cc/bbs/FITNESS/M.1670330731.A.7FA.html'\n",
    "# print(article(arturl))\n",
    "    \n",
    "def geturl(url):\n",
    "    #建立一個 Resquest 物件，附加 Request Headers 的資訊\n",
    "    request = req.Request(url, headers=\n",
    "    {\"cookie\" : \"over18=1\",\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\"})\n",
    "    with req.urlopen(request) as response:\n",
    "        data = response.read().decode(\"utf-8\")\n",
    "    root = bs4.BeautifulSoup(data,\"html.parser\")#讓Beautifulsoup 協助我們解析 HTML 格式文件\n",
    "    titles= root.find_all(\"div\", class_=\"title\")# 尋找所有 class=\"title\" 的 div 標籤    \n",
    "    articles =[]\n",
    "    for title in titles:\n",
    "        if title.a != None: # 如果標題包含 a 標籤（\"沒有被刪除\"），增加標題網址進articles\n",
    "            articles.append(\"https://www.ptt.cc\"+title.a[\"href\"])\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef4b8e",
   "metadata": {},
   "source": [
    "## 文章\n",
    "修改 pageurl: bbs/版名/index\n",
    "\n",
    "### 文章要儲存為json檔！！！<br>\n",
    "修改 with open 的路徑和檔名（絕對路徑前面要加 r'path'）:<br>\n",
    "with open(\"路徑＋檔名\", \"a\") as f: （如果不加路徑會直接存到同個資料夾裡）\n",
    "\n",
    "\n",
    "\n",
    "except 儲存錯誤處理檔案為txt檔，是漏掉沒加進去的 url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a582b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alllist = []\n",
    "\n",
    "for i in range(1,5):\n",
    "    pageurl = \"https://www.ptt.cc/bbs/FITNESS/index\"+str(i)+\".html\"\n",
    "    for arturl in geturl(pageurl):\n",
    "        try:\n",
    "            alllist.append(article(arturl))\n",
    "            print(time.ctime(),arturl+\" is done\")\n",
    "        except:\n",
    "            with open(\"/Users/guanlizhe/Desktop/code/python/web_crawer/ptt_/kuan/art_error.txt\", \"a\") as f:\n",
    "                f.write(arturl+'\\n')\n",
    "            print(\"Stop at \"+arturl)\n",
    "    print(time.ctime(),\"\\n\",pageurl+\" is done\")\n",
    "with open(\"/Users/guanlizhe/Desktop/code/python/web_crawer/ptt_/kuan/art_index.json\", \"a\") as f:\n",
    "    json.dump(alllist, f, ensure_ascii=False,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c65e4d",
   "metadata": {},
   "source": [
    "## 文章-處理漏掉的網址\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec2b8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_url = []\n",
    "with open(\"/Users/guanlizhe/Desktop/code/python/web_crawer/ptt_/kuan/art_error.txt\") as f:\n",
    "    url = f.read()\n",
    "    urllist = url.split(\"\\n\")\n",
    "urllist = [i for i in urllist if i != '']\n",
    "\n",
    "\n",
    "for arturl in urllist:\n",
    "        error_url.append(article(arturl))\n",
    "        print(time.ctime(),arturl+\" is done\")\n",
    "with open(\"/Users/guanlizhe/Desktop/code/python/web_crawer/ptt_/kuan/art_index.json\", \"a\") as f:\n",
    "    json.dump(error_url, f, ensure_ascii=False,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22150e0",
   "metadata": {},
   "source": [
    "## 留言\n",
    "修改 pageurl: bbs/版名/index\n",
    "\n",
    "### 留言要儲存為csv ！！！\n",
    "修改 with open 的路徑和檔名（絕對路徑前面要加 r'path'）:<br>\n",
    "with open(\"路徑＋檔名\", \"a\") as f: （如果不加路徑會直接存到同個資料夾裡）\n",
    "\n",
    "except 儲存錯誤處理檔案為txt檔，是漏掉沒加進去的 url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "082cb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9,11):\n",
    "    pageurl = \"https://www.ptt.cc/bbs/FITNESS/index\"+str(i)+\".html\"\n",
    "    for arturl in geturl(pageurl):\n",
    "        try:\n",
    "            (pd.DataFrame.from_dict(data=comments(arturl))\n",
    "                .to_csv('/Users/guanlizhe/Desktop/code/python/web_crawer/ptt_/kuan/com_index.csv', mode='a'))\n",
    "            print(time.ctime(),arturl+\" is done\")\n",
    "        except:\n",
    "            with open(\"/Users/guanlizhe/Desktop/code/python/web_crawer/ptt_/kuan/com_error.txt\", \"a\") as f:\n",
    "                f.write(arturl+'\\n')\n",
    "            print(\"Stop at \"+arturl)\n",
    "    print(time.ctime(),\"\\n\",pageurl+\" is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ec95b",
   "metadata": {},
   "source": [
    "## 留言-處理漏掉的網址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88ddc916",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/guanlizhe/Desktop/code/python/web_crawer/ptt_/kuan/com_error.txt\") as f:\n",
    "    url = f.read()\n",
    "    urllist = url.split(\"\\n\")\n",
    "urllist = [i for i in urllist if i != '']\n",
    "\n",
    "for url in urllist:\n",
    "    (pd.DataFrame.from_dict(data=comments(url))\n",
    "        .to_csv('/Users/guanlizhe/Desktop/code/python/web_crawer/ptt_/kuan/com_index.csv', mode='a'))\n",
    "    print(time.ctime(),url+\" is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc84bd1",
   "metadata": {},
   "source": [
    "## 清理重複留言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5587fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"/Users/guanlizhe/Desktop/code/python/web_crawer/ptt_/kuan/com_index.csv\", index_col=[0])\n",
    "\n",
    "df=data.drop_duplicates()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "(pd.DataFrame.from_dict(data=df)\n",
    "     .to_csv(\"/Users/guanlizhe/Desktop/code/python/web_crawer/ptt_/kuan/com_index.csv\", mode='w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73952795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"/Users/guanlizhe/Desktop/code/python/web_crawer/ptt_/ptt_data/Gossiping/index0-1.csv\", index_col=[0],low_memory=False)\n",
    "print(type(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
